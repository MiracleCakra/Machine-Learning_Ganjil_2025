{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiracleCakra/Machine-Learning_Ganjil_2025/blob/main/Week07_JS07/Praktikum06_JS07_ANN_ANNOY%2C_FAISS%2C_dan_HNSWLIB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2d9a28",
      "metadata": {
        "id": "8f2d9a28"
      },
      "source": [
        "# Praktikum 6\n",
        "\n",
        "Lakukan percobaan penggunaan ANNOY, FAISS, dan HNSWLIB pada dataset sekunder berukuran besar (Micro Spotify) pada link berikut: https://www.kaggle.com/datasets/bwandowando/spotify-songs-with-attributes-and-lyrics/data . Download data dan load CSV filenya (pilih dataset yg pertama dari dua dataset). pilih hanya fitur numerik saja, dan lakukan normalisasi menggunakan StandardScaler. Lakukan pencarian track terdekat dan bandingkan hasilnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3bc0f8ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bc0f8ca",
        "outputId": "f0f3101c-0869-4a26-c26f-3f1d72180ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'spotify-songs-with-attributes-and-lyrics' dataset.\n",
            "Path to dataset files: /kaggle/input/spotify-songs-with-attributes-and-lyrics\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bwandowando/spotify-songs-with-attributes-and-lyrics\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "!pip install -q annoy faiss-cpu hnswlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17fa3030",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17fa3030",
        "outputId": "fccbd046-7342-4abd-82e1-d6bf39e4d05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact NN (queries=1000) done in 3811.770 s\n",
            "Annoy build: 76.306 s, query all: 272.183 s\n",
            "HNSW build: 177.083 s, query all: 144.185 s\n",
            "FAISS build: 0.356 s, query all: 715.025 s\n",
            "\n",
            "Summary (build time | query time for sampled points | recall@k)\n",
            "Exact:  - | 3811.770 s (queries only) | recall=1.00\n",
            "Annoy:  76.306 s | 272.183 s | recall@10=0.9945\n",
            "HNSW:   177.083 s | 144.185 s | recall@10=0.9955\n",
            "FAISS:  0.356 s | 715.025 s | recall@10=0.9982\n",
            "\n",
            "Top-5 neighbors for first sampled query (dataset index = 287796)\n",
            "Exact NN: [     0 394553 764272 837727 749223]\n",
            "Annoy:    [0, 394553, 764272, 837727, 749223]\n",
            "HNSW:     [     0 394553 764272 837727 749223]\n",
            "FAISS:    [     0 394553 764272 837727 749223]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import faiss\n",
        "from annoy import AnnoyIndex\n",
        "import hnswlib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Use all available CPU cores where possible\n",
        "n_cores = os.cpu_count() or 1\n",
        "os.environ.setdefault('OMP_NUM_THREADS', str(n_cores))\n",
        "os.environ.setdefault('OPENBLAS_NUM_THREADS', str(n_cores))\n",
        "os.environ.setdefault('MKL_NUM_THREADS', str(n_cores))\n",
        "# Tell faiss to use multiple threads (if built with OpenMP)\n",
        "try:\n",
        "    faiss.omp_set_num_threads(n_cores)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# -------------------------------\n",
        "# Load dataset (drop NaNs in chosen features)\n",
        "# -------------------------------\n",
        "df = pd.read_csv(f'{path}/songs_with_attributes_and_lyrics.csv')  # ganti path sesuai lokasi file\n",
        "features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
        "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "df = df[features].dropna().reset_index(drop=True)\n",
        "X = df.values\n",
        "\n",
        "# Standardize and cast to float32 (required by faiss/hnswlib)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X).astype(np.float32)\n",
        "\n",
        "n = X_scaled.shape[0]\n",
        "k = 10  # jumlah nearest neighbors\n",
        "# To keep this runnable on limited RAM, sample up to 1000 query points\n",
        "n_queries = min(1000, n)\n",
        "rng = np.random.default_rng(42)\n",
        "query_idx = rng.choice(n, size=n_queries, replace=False)\n",
        "# Xq = X_scaled[query_idx]\n",
        "Xq = X_scaled\n",
        "\n",
        "# -------------------------------\n",
        "# Exact Nearest Neighbor (brute-force) - only for the sampled queries\n",
        "# -------------------------------\n",
        "t0 = time.time()\n",
        "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean', n_jobs=-1)\n",
        "nn.fit(X_scaled)\n",
        "dist_exact, idx_exact = nn.kneighbors(Xq)\n",
        "time_exact = time.time() - t0\n",
        "print(f\"Exact NN (queries={n_queries}) done in {time_exact:.3f} s\")\n",
        "\n",
        "# -------------------------------\n",
        "# Annoy (build + query on sampled points)\n",
        "# -------------------------------\n",
        "t0 = time.time()\n",
        "fdim = X_scaled.shape[1]\n",
        "index_annoy = AnnoyIndex(fdim, 'euclidean')\n",
        "for i, v in enumerate(X_scaled):\n",
        "    index_annoy.add_item(i, v.tolist())\n",
        "n_trees = 50\n",
        "index_annoy.build(n_trees)\n",
        "t_build_annoy = time.time() - t0\n",
        "\n",
        "tq = time.time()\n",
        "# Annoy: parallelize queries using joblib (threading) to utilize multiple cores\n",
        "def _query_annoy(v):\n",
        "    return index_annoy.get_nns_by_vector(v.tolist(), k)\n",
        "idx_annoy = Parallel(n_jobs=n_cores, prefer='threads')(delayed(_query_annoy)(v) for v in Xq)\n",
        "time_query_annoy = time.time() - tq\n",
        "print(f\"Annoy build: {t_build_annoy:.3f} s, query all: {time_query_annoy:.3f} s\")\n",
        "\n",
        "# -------------------------------\n",
        "# HNSW (hnswlib)\n",
        "# -------------------------------\n",
        "t0 = time.time()\n",
        "p = hnswlib.Index(space='l2', dim=fdim)\n",
        "p.init_index(max_elements=n, ef_construction=200, M=16)\n",
        "p.add_items(X_scaled)\n",
        "p.set_ef(200)\n",
        "t_build_hnsw = time.time() - t0\n",
        "\n",
        "tq = time.time()\n",
        "# hnswlib supports num_threads in knn_query\n",
        "idx_hnsw, dist_hnsw = p.knn_query(Xq, k=k, num_threads=n_cores)\n",
        "time_query_hnsw = time.time() - tq\n",
        "print(f\"HNSW build: {t_build_hnsw:.3f} s, query all: {time_query_hnsw:.3f} s\")\n",
        "\n",
        "# -------------------------------\n",
        "# FAISS IVF (train on full set, query sampled points)\n",
        "# -------------------------------\n",
        "t0 = time.time()\n",
        "quantizer = faiss.IndexFlatL2(fdim)\n",
        "nlist = 100\n",
        "index_faiss = faiss.IndexIVFFlat(quantizer, fdim, nlist, faiss.METRIC_L2)\n",
        "# FAISS requires float32 and contiguous arrays\n",
        "index_faiss.train(np.ascontiguousarray(X_scaled))\n",
        "index_faiss.add(np.ascontiguousarray(X_scaled))\n",
        "index_faiss.nprobe = 10\n",
        "t_build_faiss = time.time() - t0\n",
        "\n",
        "tq = time.time()\n",
        "# FAISS can use multiple threads via set_num_threads if available\n",
        "try:\n",
        "    faiss.omp_set_num_threads(n_cores)\n",
        "except Exception:\n",
        "    pass\n",
        "D_faiss, idx_faiss = index_faiss.search(np.ascontiguousarray(Xq), k)\n",
        "time_query_faiss = time.time() - tq\n",
        "print(f\"FAISS build: {t_build_faiss:.3f} s, query all: {time_query_faiss:.3f} s\")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate recall@k for each ANN vs exact\n",
        "# -------------------------------\n",
        "def recall_at_k(true_idx, pred_idx, k):\n",
        "    # true_idx: (n_queries, k), pred_idx: iterable of length n_queries with lists/arrays\n",
        "    total = 0.0\n",
        "    n = len(true_idx)\n",
        "    for t, p in zip(true_idx, pred_idx):\n",
        "        pset = set(p.tolist() if hasattr(p, 'tolist') else p)\n",
        "        total += len(pset.intersection(set(t[:k]))) / float(k)\n",
        "    return total / n\n",
        "\n",
        "rec_annoy = recall_at_k(idx_exact, idx_annoy, k)\n",
        "rec_hnsw = recall_at_k(idx_exact, idx_hnsw, k)\n",
        "rec_faiss = recall_at_k(idx_exact, idx_faiss, k)\n",
        "\n",
        "print('\\nSummary (build time | query time for sampled points | recall@k)')\n",
        "print(f\"Exact:  - | {time_exact:.3f} s (queries only) | recall=1.00\")\n",
        "print(f\"Annoy:  {t_build_annoy:.3f} s | {time_query_annoy:.3f} s | recall@{k}={rec_annoy:.4f}\")\n",
        "print(f\"HNSW:   {t_build_hnsw:.3f} s | {time_query_hnsw:.3f} s | recall@{k}={rec_hnsw:.4f}\")\n",
        "print(f\"FAISS:  {t_build_faiss:.3f} s | {time_query_faiss:.3f} s | recall@{k}={rec_faiss:.4f}\")\n",
        "\n",
        "# show top-5 neighbors for the first sampled query (original dataset index)\n",
        "qid = query_idx[0]\n",
        "print(\"\\nTop-5 neighbors for first sampled query (dataset index = {})\".format(int(qid)))\n",
        "print(f\"Exact NN: {idx_exact[0][:5]}\")\n",
        "print(f\"Annoy:    {idx_annoy[0][:5]}\")\n",
        "print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n",
        "print(f\"FAISS:    {idx_faiss[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90a4cf28"
      },
      "source": [
        "## Analisa Code dan Hasil Percobaan Nearest Neighbor Search\n",
        "\n",
        "Kode ini bertujuan untuk membandingkan performa dari beberapa algoritma Approximate Nearest Neighbor (ANN) Search, yaitu Annoy, HNSWLib, dan FAISS, pada dataset lagu Spotify yang berukuran besar. Perbandingan dilakukan berdasarkan kecepatan waktu build index, kecepatan waktu query (pencarian tetangga terdekat), dan akurasi (recall@k). Algoritma brute-force (Exact Nearest Neighbor) juga disertakan sebagai baseline untuk membandingkan akurasi, karena hasil dari brute-force adalah hasil yang paling akurat (recall@k = 1.00).\n",
        "\n",
        "#### Analisa Hasil:\n",
        "\n",
        "Berdasarkan output yang dihasilkan:\n",
        "\n",
        "| Algoritma    | Waktu Build (s) | Waktu Query (s) | Recall@10 |\n",
        "| :----------- | :-------------- | :-------------- | :-------- |\n",
        "| **Exact NN** | -               | 3811.770        | 1.00      |\n",
        "| **Annoy**    | 76.306          | 272.183         | 0.9945    |\n",
        "| **HNSW**     | 177.083         | 144.185         | 0.9955    |\n",
        "| **FAISS IVF**| 0.356           | 715.025         | 0.9982    |\n",
        "\n",
        "**Interpretasi Hasil:**\n",
        "\n",
        "*   **Exact NN**: Seperti yang diperkirakan, Exact NN memiliki waktu query yang sangat lama (3811.770 detik) karena melakukan perbandingan dengan setiap titik data. Ini menunjukkan bahwa brute-force tidak praktis untuk dataset berukuran besar dalam skenario yang membutuhkan respon cepat. Namun, akurasinya sempurna (recall@10 = 1.00), menjadikannya benchmark untuk algoritma ANN.\n",
        "\n",
        "*   **Annoy**: Annoy menunjukkan waktu build yang relatif cepat (76.306 detik) dan waktu query yang cukup baik (272.183 detik). Akurasinya sangat tinggi (recall@10 = 0.9945), yang berarti Annoy berhasil menemukan sebagian besar tetangga terdekat yang sebenarnya. Annoy dikenal karena efisiensinya dalam penggunaan memori dan waktu build yang cepat, menjadikannya pilihan yang baik untuk skenario di mana index perlu sering diperbarui atau memori terbatas.\n",
        "\n",
        "*   **HNSW**: HNSW memiliki waktu build yang lebih lama dibandingkan Annoy (177.083 detik) tetapi menunjukkan waktu query tercepat di antara algoritma ANN (144.185 detik). Akurasinya juga sangat tinggi (recall@10 = 0.9955), bahkan sedikit lebih tinggi dari Annoy pada konfigurasi ini. HNSW umumnya dianggap sebagai salah satu algoritma ANN paling canggih, menawarkan keseimbangan yang sangat baik antara kecepatan query dan akurasi.\n",
        "\n",
        "*   **FAISS IVF**: FAISS IVF menunjukkan waktu build yang sangat cepat (0.356 detik), jauh lebih cepat dibandingkan Annoy dan HNSW. Akurasinya juga yang tertinggi di antara ANN (recall@10 = 0.9982). Namun, waktu query-nya paling lambat di antara ANN (715.025 detik) pada konfigurasi `nprobe=10`. Ini menunjukkan bahwa FAISS IVF sangat efisien dalam membangun index, tetapi kecepatan query-nya sangat bergantung pada parameter `nprobe`. Meningkatkan `nprobe` dapat meningkatkan akurasi tetapi akan memperlambat query. Kecepatan build yang superior membuat FAISS IVF menarik untuk skenario di mana data statis atau jarang diperbarui.\n",
        "\n",
        "**Kesimpulan:**\n",
        "\n",
        "Percobaan ini menunjukkan bahwa ketiga algoritma ANN (Annoy, HNSW, dan FAISS IVF) berhasil menemukan tetangga terdekat dengan akurasi yang sangat tinggi (recall@10 di atas 0.99) pada dataset yang besar, jauh lebih cepat dibandingkan metode brute-force.\n",
        "\n",
        "*   **HNSW** menawarkan keseimbangan terbaik antara kecepatan query dan akurasi dalam percobaan ini.\n",
        "*   **Annoy** adalah pilihan yang baik jika waktu build yang cepat dan efisiensi memori menjadi prioritas utama.\n",
        "*   **FAISS IVF** unggul dalam waktu build dan akurasi (dengan tuning `nprobe`), tetapi waktu query-nya perlu dioptimalkan lebih lanjut tergantung kebutuhan."
      ],
      "id": "90a4cf28"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}